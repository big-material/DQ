{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DCC import *\n",
    "from Utils import *\n",
    "from Plots import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "init_plotting()\n",
    "\n",
    "dataset2name = {\n",
    "    \"Bala_classification_dataset.csv\": \"Bala Classification\",\n",
    "    \"Bala_regression_dataset.csv\": \"Bala Regression\",\n",
    "    \"bandgap.csv\": \"Bandgap\",\n",
    "    \"BMDS_data.csv\": \"BMDS\",\n",
    "    \"Crystal_structure.csv\": \"Crystal Structure\",\n",
    "    \"Glass.csv\": \"Glass\",\n",
    "    \"PUE.csv\": \"PUE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "class PerturbationType(Enum):\n",
    "    Deletion = \"Deletion\"\n",
    "    AdditionLinear = \"AdditionLinear\"\n",
    "    AdditionRand = \"AdditionRand\"\n",
    "    ReplacementLinear = \"ReplacementLinear\"\n",
    "    ReplacementRand = \"ReplacementRand\"\n",
    "\n",
    "\n",
    "def generate_random_according_df(\n",
    "    n: int, m: int, df: pd.DataFrame, task_type=None, target_col=None\n",
    "):\n",
    "    cols = df.columns\n",
    "    new_data = []\n",
    "    for i in range(n):\n",
    "        new_row = []\n",
    "        for j in range(m):\n",
    "            new_row.append(random.uniform(df[cols[j]].min(), df[cols[j]].max()))\n",
    "        new_data.append(new_row)\n",
    "    new_df = pd.DataFrame(new_data, columns=cols)\n",
    "    if task_type == \"classification\":\n",
    "        if target_col is not None:\n",
    "            new_df[target_col] = np.random.choice(\n",
    "                df[target_col].unique(), size=n, replace=True\n",
    "            )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def generate_linear_according_df(\n",
    "    n: int, m: int, df: pd.DataFrame, task_type=None, target_col=None\n",
    "):\n",
    "    if task_type == \"classification\":\n",
    "        # Generate a classification dataset\n",
    "        X, y = make_classification(\n",
    "            n_samples=n,\n",
    "            n_features=m - 1,\n",
    "            n_classes=len(df[target_col].unique()),\n",
    "            n_informative= (m - 1) // 2,\n",
    "        )[:2]\n",
    "    else:\n",
    "        X, y = make_regression(\n",
    "            n_samples=n, n_features=m - 1, noise=0.1\n",
    "        )[:2]\n",
    "\n",
    "    X_cols = [x for x in df.columns if x != target_col]\n",
    "    df_linear = pd.DataFrame(X, columns=X_cols)\n",
    "    df_linear[target_col] = y\n",
    "    # normalize the data to the range of df\n",
    "    for col in df.columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        df_linear[col] = (df_linear[col] - df_linear[col].min()) / (\n",
    "            df_linear[col].max() - df_linear[col].min()\n",
    "        ) * (max_val - min_val) + min_val\n",
    "\n",
    "    return df_linear\n",
    "\n",
    "\n",
    "def perturbate(\n",
    "    data: pd.DataFrame,\n",
    "    ptb_type: PerturbationType,\n",
    "    task_type,\n",
    "    target_col,\n",
    "    ratio=0.1,\n",
    "):\n",
    "    n, m = data.shape\n",
    "    if ptb_type == PerturbationType.Deletion:\n",
    "        perturbed_data = data.sample(frac=1 - ratio)\n",
    "    elif ptb_type == PerturbationType.AdditionLinear:\n",
    "        df_linear = generate_linear_according_df(\n",
    "            n // 20, m, data, task_type, target_col\n",
    "        )\n",
    "        perturbed_data = pd.concat([data, df_linear], ignore_index=True)\n",
    "    elif ptb_type == PerturbationType.AdditionRand:\n",
    "        df_rand = generate_random_according_df(n // 20, m, data, task_type, target_col)\n",
    "        perturbed_data = pd.concat([data, df_rand], ignore_index=True)\n",
    "    elif ptb_type == PerturbationType.ReplacementLinear:\n",
    "        df_linear = generate_linear_according_df(\n",
    "            n // 20, m, data, task_type, target_col\n",
    "        )\n",
    "        perturbed_data = random_replace_rows(data, df_linear)\n",
    "    elif ptb_type == PerturbationType.ReplacementRand:\n",
    "        df_rand = generate_random_according_df(n // 20, m, data, task_type, target_col)\n",
    "        perturbed_data = random_replace_rows(data, df_rand)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown perturbation type: {ptb_type}\")\n",
    "\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from joblib import Parallel, delayed\n",
    "import shap\n",
    "\n",
    "dataset_dir = Path(\"processed_data\")\n",
    "datasets = list(dataset_dir.glob(\"*.csv\"))\n",
    "dataset_config = {\n",
    "    \"Bala_classification_dataset.csv\": {\n",
    "        \"target_col\": \"Formability\",\n",
    "        \"type\": \"classification\",\n",
    "    },\n",
    "    \"Bala_regression_dataset.csv\": {\n",
    "        \"target_col\": \"Ferroelectric_Tc_in_Kelvin\",\n",
    "        \"type\": \"regression\",\n",
    "    },\n",
    "    \"bandgap.csv\": {\n",
    "        \"target_col\": \"target\",\n",
    "        \"type\": \"regression\",\n",
    "    },\n",
    "    \"BMDS_data.csv\": {\n",
    "        \"target_col\": \"soc_bandgap\",\n",
    "        \"type\": \"regression\",\n",
    "    },\n",
    "    \"Crystal_structure.csv\": {\n",
    "        \"target_col\": \"Lowest distortion\",\n",
    "        \"type\": \"classification\",\n",
    "    },\n",
    "    \"Glass.csv\": {\n",
    "        \"target_col\": \"Type of glass\",\n",
    "        \"type\": \"classification\",\n",
    "    },\n",
    "    \"PUE.csv\": {\n",
    "        \"target_col\": \"logYM\",\n",
    "        \"type\": \"regression\",\n",
    "    },\n",
    "}\n",
    "def get_shap_values(df, target_col, type):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    if type == \"regression\":\n",
    "        model = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "    elif type == \"classification\":\n",
    "        model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"type must be either 'regression' or 'classification'\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    # shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "    if type == \"classification\":\n",
    "        # For classification, shap_values is a list of arrays, one for each class\n",
    "        # We take the mean absolute value across all classes\n",
    "        mean_abs_shap = np.mean(np.mean(np.abs(shap_values), axis=0), axis=1)\n",
    "    else:\n",
    "        mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "    # print(f\"Mean absolute SHAP values: {mean_abs_shap}\")\n",
    "    feats2shap = {col: mean_abs_shap[i] for i, col in enumerate(X.columns)}\n",
    "    return feats2shap\n",
    "\n",
    "\n",
    "def get_corr_info(df, corr_func, target_col):\n",
    "    corr_matrix = corr_func(df)\n",
    "    feats2corr = {}\n",
    "    for col in df.columns:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        feats2corr[col] = corr_matrix.loc[target_col, col]\n",
    "    return feats2corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    logger = logging.getLogger(\"worker\")\n",
    "    if not logger.hasHandlers():\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter('%(process)d - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "# Define correlation functions and perturbation ratios\n",
    "correlation_functions = [\n",
    "    pearson_matrix,\n",
    "    spearman_matrix,\n",
    "    kendall_matrix,\n",
    "    mutual_info_matrix,\n",
    "    js_corr_matrix,\n",
    "    wd_corr_matrix,\n",
    "    xi_matrix,\n",
    "    dcor_matrix,\n",
    "]\n",
    "\n",
    "def process_single_dataset(dataset_path, ptb_ratio):\n",
    "    \"\"\"Process a single dataset with given perturbation ratio and correlation function.\"\"\"\n",
    "    logger = get_logger()\n",
    "    dataset_results = {}\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "        dataset_name = dataset_path.name\n",
    "        target_col = dataset_config[dataset_name][\"target_col\"]\n",
    "        task_type = dataset_config[dataset_name][\"type\"]\n",
    "        \n",
    "        # Calculate original SHAP values\n",
    "        original_shap = get_shap_values(df, target_col, task_type)\n",
    "        \n",
    "        dataset_results[\"ori_feats2shap\"] = original_shap\n",
    "        dataset_results[\"name\"] = dataset2name[dataset_name]\n",
    "        dataset_results[\"type\"] = task_type\n",
    "        dataset_results[\"target_col\"] = target_col\n",
    "        \n",
    "        # Initialize perturbation result containers\n",
    "        for ptb_type in PerturbationType:\n",
    "            dataset_results[ptb_type.value] = {}\n",
    "        \n",
    "        # Process each perturbation type\n",
    "        for ptb_type in PerturbationType:\n",
    "            modified_df = perturbate(\n",
    "                df, ptb_type, task_type, target_col, \n",
    "                ratio=ptb_ratio\n",
    "            )\n",
    "            \n",
    "            perturbed_shap = get_shap_values(modified_df, target_col, task_type)\n",
    "            \n",
    "            # dataset_results[ptb_type.value][\"dcc\"] = dcc_result\n",
    "            dataset_results[ptb_type.value][\"feats2shap\"] = perturbed_shap\n",
    "            for corr_func in correlation_functions:\n",
    "                correlation_info = get_corr_info(modified_df, corr_func, target_col)\n",
    "                dataset_results[ptb_type.value][f\"feats2corr_{corr_func.__name__}\"] = correlation_info\n",
    "                dataset_results[ptb_type.value][f\"dcc_{corr_func.__name__}\"] = dcc_diff_features(df, modified_df, target_col, eps=0.04, corr_func=corr_func)\n",
    "            \n",
    "        return dataset_name, dataset_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {dataset_path.name}: {str(e)}\")\n",
    "        return dataset_path.name, None\n",
    "\n",
    "def process_ratio(ptb_ratio):\n",
    "    \"\"\"Process all datasets for a specific ratio and correlation function combination.\"\"\"\n",
    "    \n",
    "    logger = get_logger()\n",
    "    \n",
    "    dataset_paths = list(Path(\"processed_data\").glob(\"*.csv\"))\n",
    "    \n",
    "    # Parallel processing of datasets\n",
    "    dataset_results = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(process_single_dataset)(dataset_path, ptb_ratio) \n",
    "        for dataset_path in dataset_paths\n",
    "    )\n",
    "    # Consolidate results\n",
    "    consolidated_results = defaultdict(dict)\n",
    "    for dataset_name, result_data in dataset_results:\n",
    "        if result_data is not None:\n",
    "            consolidated_results[dataset_name] = result_data\n",
    "    \n",
    "    # Save consolidated results\n",
    "    output_dir = Path(\"results\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    output_file = output_dir / f\"SHAP_results_{ptb_ratio}.pkl\"\n",
    "    \n",
    "    with open(output_file, \"wb\") as file_handle:\n",
    "        pickle.dump(dict(consolidated_results), file_handle)\n",
    "    \n",
    "    logger.info(f\"Saved results to {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_single_dataset(Path(\"processed_data/PUE.csv\"), 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  11 | elapsed: 79.9min remaining: 359.6min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  11 | elapsed: 81.7min remaining: 30.6min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed: 82.0min finished\n"
     ]
    }
   ],
   "source": [
    "perturbation_ratios = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15]\n",
    "\n",
    "\n",
    "# Parallel processing of ratio-correlation combinations\n",
    "# Using fewer jobs for outer loop to avoid overwhelming the system\n",
    "processed_files = Parallel(n_jobs=-1, verbose=2)(\n",
    "    delayed(process_ratio)(ratio)\n",
    "    for ratio in perturbation_ratios\n",
    ")\n",
    "\n",
    "logging.info(f\"All processing complete. Generated {len(processed_files)} result files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.05\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.05\n",
      "Processing dataset: bandgap.csv with ratio: 0.05\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.05\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.05\n",
      "Processing dataset: Glass.csv with ratio: 0.05\n",
      "Processing dataset: PUE.csv with ratio: 0.05\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.06\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.06\n",
      "Processing dataset: bandgap.csv with ratio: 0.06\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.06\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.06\n",
      "Processing dataset: Glass.csv with ratio: 0.06\n",
      "Processing dataset: PUE.csv with ratio: 0.06\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.07\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.07\n",
      "Processing dataset: bandgap.csv with ratio: 0.07\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.07\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.07\n",
      "Processing dataset: Glass.csv with ratio: 0.07\n",
      "Processing dataset: PUE.csv with ratio: 0.07\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.08\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.08\n",
      "Processing dataset: bandgap.csv with ratio: 0.08\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.08\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.08\n",
      "Processing dataset: Glass.csv with ratio: 0.08\n",
      "Processing dataset: PUE.csv with ratio: 0.08\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.09\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.09\n",
      "Processing dataset: bandgap.csv with ratio: 0.09\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.09\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.09\n",
      "Processing dataset: Glass.csv with ratio: 0.09\n",
      "Processing dataset: PUE.csv with ratio: 0.09\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.1\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.1\n",
      "Processing dataset: bandgap.csv with ratio: 0.1\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.1\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.1\n",
      "Processing dataset: Glass.csv with ratio: 0.1\n",
      "Processing dataset: PUE.csv with ratio: 0.1\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.11\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.11\n",
      "Processing dataset: bandgap.csv with ratio: 0.11\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.11\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.11\n",
      "Processing dataset: Glass.csv with ratio: 0.11\n",
      "Processing dataset: PUE.csv with ratio: 0.11\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.12\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.12\n",
      "Processing dataset: bandgap.csv with ratio: 0.12\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.12\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.12\n",
      "Processing dataset: Glass.csv with ratio: 0.12\n",
      "Processing dataset: PUE.csv with ratio: 0.12\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.13\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.13\n",
      "Processing dataset: bandgap.csv with ratio: 0.13\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.13\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.13\n",
      "Processing dataset: Glass.csv with ratio: 0.13\n",
      "Processing dataset: PUE.csv with ratio: 0.13\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.14\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.14\n",
      "Processing dataset: bandgap.csv with ratio: 0.14\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.14\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.14\n",
      "Processing dataset: Glass.csv with ratio: 0.14\n",
      "Processing dataset: PUE.csv with ratio: 0.14\n",
      "Processing dataset: Bala_classification_dataset.csv with ratio: 0.15\n",
      "Processing dataset: Bala_regression_dataset.csv with ratio: 0.15\n",
      "Processing dataset: bandgap.csv with ratio: 0.15\n",
      "Processing dataset: BMDS_data.csv with ratio: 0.15\n",
      "Processing dataset: Crystal_structure.csv with ratio: 0.15\n",
      "Processing dataset: Glass.csv with ratio: 0.15\n",
      "Processing dataset: PUE.csv with ratio: 0.15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>perturbation_type</th>\n",
       "      <th>perturbation_ratio</th>\n",
       "      <th>SHAP_feat</th>\n",
       "      <th>SHAP_feat_ratio</th>\n",
       "      <th>max_shap_feat</th>\n",
       "      <th>feat</th>\n",
       "      <th>DCC_pearson_matrix</th>\n",
       "      <th>Corr_pearson_matrix</th>\n",
       "      <th>DCC_spearman_matrix</th>\n",
       "      <th>...</th>\n",
       "      <th>DCC_mutual_info_matrix</th>\n",
       "      <th>Corr_mutual_info_matrix</th>\n",
       "      <th>DCC_js_corr_matrix</th>\n",
       "      <th>Corr_js_corr_matrix</th>\n",
       "      <th>DCC_wd_corr_matrix</th>\n",
       "      <th>Corr_wd_corr_matrix</th>\n",
       "      <th>DCC_xi_matrix</th>\n",
       "      <th>Corr_xi_matrix</th>\n",
       "      <th>DCC_dcor_matrix</th>\n",
       "      <th>Corr_dcor_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bala_classification_dataset.csv</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.297431</td>\n",
       "      <td>Mendeleev_Number</td>\n",
       "      <td>Compound</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.100841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.527515</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.180511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bala_classification_dataset.csv</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.027763</td>\n",
       "      <td>0.262175</td>\n",
       "      <td>Mendeleev_Number</td>\n",
       "      <td>x(BiMe1Me2)O3</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-0.395085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.077559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.568063</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.229435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bala_classification_dataset.csv</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.036863</td>\n",
       "      <td>0.348111</td>\n",
       "      <td>Mendeleev_Number</td>\n",
       "      <td>Me1</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-0.094558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.320797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437985</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.204973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bala_classification_dataset.csv</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.026647</td>\n",
       "      <td>0.251633</td>\n",
       "      <td>Mendeleev_Number</td>\n",
       "      <td>Me2</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>-0.246314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.420690</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.119355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bala_classification_dataset.csv</td>\n",
       "      <td>Deletion</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.009097</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>Mendeleev_Number</td>\n",
       "      <td>frac-Me1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902350</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-0.002957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           dataset perturbation_type  perturbation_ratio  \\\n",
       "0  Bala_classification_dataset.csv          Deletion                0.05   \n",
       "1  Bala_classification_dataset.csv          Deletion                0.05   \n",
       "2  Bala_classification_dataset.csv          Deletion                0.05   \n",
       "3  Bala_classification_dataset.csv          Deletion                0.05   \n",
       "4  Bala_classification_dataset.csv          Deletion                0.05   \n",
       "\n",
       "   SHAP_feat  SHAP_feat_ratio     max_shap_feat           feat  \\\n",
       "0   0.031496         0.297431  Mendeleev_Number       Compound   \n",
       "1   0.027763         0.262175  Mendeleev_Number  x(BiMe1Me2)O3   \n",
       "2   0.036863         0.348111  Mendeleev_Number            Me1   \n",
       "3   0.026647         0.251633  Mendeleev_Number            Me2   \n",
       "4   0.009097         0.085910  Mendeleev_Number       frac-Me1   \n",
       "\n",
       "   DCC_pearson_matrix  Corr_pearson_matrix  DCC_spearman_matrix  ...  \\\n",
       "0            1.000000            -0.100841                  1.0  ...   \n",
       "1            0.928571            -0.395085                  1.0  ...   \n",
       "2            0.928571            -0.094558                  1.0  ...   \n",
       "3            0.928571            -0.246314                  1.0  ...   \n",
       "4            1.000000             0.017028                  1.0  ...   \n",
       "\n",
       "   DCC_mutual_info_matrix  Corr_mutual_info_matrix  DCC_js_corr_matrix  \\\n",
       "0                1.000000                 0.024555                 1.0   \n",
       "1                0.928571                 0.077559                 1.0   \n",
       "2                1.000000                 0.091084                 1.0   \n",
       "3                1.000000                 0.025397                 1.0   \n",
       "4                1.000000                 0.012468                 1.0   \n",
       "\n",
       "   Corr_js_corr_matrix  DCC_wd_corr_matrix  Corr_wd_corr_matrix  \\\n",
       "0             0.313147                 1.0             0.527515   \n",
       "1             0.528265                 1.0             0.568063   \n",
       "2             0.320797                 1.0             0.437985   \n",
       "3             0.390185                 1.0             0.420690   \n",
       "4             0.547022                 1.0             0.902350   \n",
       "\n",
       "   DCC_xi_matrix  Corr_xi_matrix  DCC_dcor_matrix  Corr_dcor_matrix  \n",
       "0       0.357143        0.180511              1.0          0.135073  \n",
       "1       0.500000        0.229435              1.0          0.377583  \n",
       "2       0.214286        0.204973              1.0          0.175186  \n",
       "3       0.214286        0.119355              1.0          0.253483  \n",
       "4       0.142857       -0.002957              1.0          0.074559  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbation_ratios = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15]\n",
    "rows = []\n",
    "for ratio in perturbation_ratios:\n",
    "    file_path = Path(f\"results/SHAP_results_{ratio}.pkl\")\n",
    "    if file_path.exists():\n",
    "        with open(file_path, \"rb\") as file_handle:\n",
    "            results = pickle.load(file_handle)\n",
    "            for dataset_name in results.keys():\n",
    "                dataset_results = results[dataset_name]\n",
    "                print(f\"Processing dataset: {dataset_name} with ratio: {ratio}\")\n",
    "                for ptb_type in PerturbationType:\n",
    "                    if ptb_type.value in dataset_results:\n",
    "\n",
    "                        feats2shap = dataset_results[ptb_type.value].get(\"feats2shap\", {})\n",
    "                        feats = list(feats2shap.keys())\n",
    "                        for feat in feats:\n",
    "                            row = {}\n",
    "                            row[\"dataset\"] = dataset_name\n",
    "                            row[\"perturbation_type\"] = ptb_type.value\n",
    "                            row[\"perturbation_ratio\"] = ratio\n",
    "                            row['SHAP_feat'] = feats2shap[feat]\n",
    "                            row['SHAP_feat_ratio'] = feats2shap[feat] / max(feats2shap.values())\n",
    "                            row['max_shap_feat'] = list(feats2shap.keys())[np.argmax(list(feats2shap.values()))]\n",
    "                            row[\"feat\"] = feat\n",
    "                            for corr_func in correlation_functions:\n",
    "                                feats2corr = dataset_results[ptb_type.value].get(f\"feats2corr_{corr_func.__name__}\", {})\n",
    "                                dcc_result = dataset_results[ptb_type.value].get(f\"dcc_{corr_func.__name__}\", {})\n",
    "                                row[f\"DCC_{corr_func.__name__}\"] = dcc_result.get(feat, None)\n",
    "                                row[f\"Corr_{corr_func.__name__}\"] = feats2corr.get(feat, None)\n",
    "                            rows.append(row)\n",
    "df_results = pd.DataFrame(rows)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', 'perturbation_type', 'perturbation_ratio', 'SHAP_feat',\n",
       "       'SHAP_feat_ratio', 'max_shap_feat', 'feat', 'DCC_pearson_matrix',\n",
       "       'Corr_pearson_matrix', 'DCC_spearman_matrix', 'Corr_spearman_matrix',\n",
       "       'DCC_kendall_matrix', 'Corr_kendall_matrix', 'DCC_mutual_info_matrix',\n",
       "       'Corr_mutual_info_matrix', 'DCC_js_corr_matrix', 'Corr_js_corr_matrix',\n",
       "       'DCC_wd_corr_matrix', 'Corr_wd_corr_matrix', 'DCC_xi_matrix',\n",
       "       'Corr_xi_matrix', 'DCC_dcor_matrix', 'Corr_dcor_matrix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results/SHAP_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
